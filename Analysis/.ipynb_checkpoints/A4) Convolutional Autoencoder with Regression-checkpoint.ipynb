{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering # 1) Agglomerative-Hierarchical\n",
    "from sklearn.cluster import KMeans                  # 2) K-Means\n",
    "from sklearn.mixture import GaussianMixture         # 3) Gaussian Mixture Models\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "from skimage.external import tifffile\n",
    "from skimage.measure import compare_mse\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA               # 2) PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load Autoencoder                                  # 3) Autoencoder (Deep dimensionality reduction)\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import utilities as myutils # Software package utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def symlink_rel(src, dst):\n",
    "    rel_path_src = os.path.relpath(src, os.path.dirname(dst))\n",
    "    os.symlink(rel_path_src, dst)\n",
    "    \n",
    "def clusterintoDirectories(labels, path, imagenamesList):\n",
    "    # remove existing subdirectories first to avoid overlap\n",
    "    sub_directories = [str(i) for i in range(8)]\n",
    "\n",
    "    for cluster in sub_directories:\n",
    "        if (cluster in os.listdir(path)) and (os.path.isdir(os.path.join(path , cluster))):\n",
    "            shutil.rmtree(os.path.join(path , cluster))\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.tif'):\n",
    "            for cluster in sub_directories: # count of distinct elements = no. of clusters\n",
    "                os.makedirs(path + '/{}'.format(cluster) , exist_ok=True)\n",
    "    \n",
    "    \n",
    "    for i in range(len(imagenamesList)):\n",
    "        # if there isnt already a symlink of this image in the coressponding subdirectory\n",
    "        if imagenamesList[i] not in os.listdir(path + '/' + sub_directories[labels[i]]): \n",
    "            symlink_rel(path + '/{}'.format(imagenamesList[i]) , \n",
    "                       path + '/{}'.format(labels[i]) + '/' + imagenamesList[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "\n",
    "train_directory = \"../../Data/Kather_all_train\"\n",
    "\n",
    "X_train = []\n",
    "train_image_names = []\n",
    "\n",
    "for filename in os.listdir(train_directory):\n",
    "    if filename.endswith('tif'):\n",
    "        image = cv2.imread(os.path.join(train_directory, filename), cv2.IMREAD_UNCHANGED)\n",
    "        resized_image = cv2.resize(image, (144, 144)) \n",
    "        X_train.append(np.asarray( resized_image, dtype=\"uint8\" ))\n",
    "        train_image_names.append(filename)\n",
    "\n",
    "y_train = myutils.loadLabelsFromsubdirectoryindex(train_image_names, \"../../Data/Kather_original_train\")\n",
    "\n",
    "# Load testing data\n",
    "\n",
    "test_directory = \"../../Data/Kather_all_test\"\n",
    "\n",
    "X_test = []\n",
    "test_image_names = []\n",
    "\n",
    "for filename in os.listdir(test_directory):\n",
    "    if filename.endswith('tif'):\n",
    "        image = cv2.imread(os.path.join(test_directory, filename), cv2.IMREAD_UNCHANGED)\n",
    "        resized_image = cv2.resize(image, (144, 144)) \n",
    "        X_test.append(np.asarray( resized_image, dtype=\"uint8\" ))\n",
    "        test_image_names.append(filename)\n",
    "\n",
    "y_test = myutils.loadLabelsFromsubdirectoryindex(test_image_names, \"../../Data/Kather_original_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "# Normalise\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CAER_MSE_KATHER = load_model('../Autoencoders/SavedModels/weights/finalists/CAER_MSE_KATHER.h5', compile=False) # 6 x 6 x 16\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'conv2d_43' # 6 x 6 x 32\n",
    "CAER_MSE_KATHER_encoder = Model(inputs=CAER_MSE_KATHER.input, outputs=CAER_MSE_KATHER.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CAER_MSE_KATHER_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200, 9, 9, 32)\n",
      "(800, 9, 9, 32)\n",
      "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100,\n",
      "                means_init=None, n_components=8, n_init=1, precisions_init=None,\n",
      "                random_state=42, reg_covar=1e-06, tol=0.001, verbose=0,\n",
      "                verbose_interval=10, warm_start=False, weights_init=None)\n",
      "0.7193166927292138\n",
      "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300, n_clusters=8,\n",
      "       n_init=10, n_jobs=None, precompute_distances='auto', random_state=0,\n",
      "       tol=0.0001, verbose=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5545186268787916"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_enc = encoder.predict(X_train)\n",
    "print(X_train_enc.shape)\n",
    "\n",
    "X_test_enc = encoder.predict(X_test)\n",
    "print(X_test_enc.shape)\n",
    "\n",
    "X_train_enc = X_train_enc.reshape(X_train_enc.shape[0] , -1) # Reshape for scaling\n",
    "X_train_enc = StandardScaler().fit_transform(X_train_enc) # Scale\n",
    "\n",
    "X_test_enc = X_test_enc.reshape(X_test_enc.shape[0] , -1) # Reshape for scaling\n",
    "X_test_enc = StandardScaler().fit_transform(X_test_enc) # Scale\n",
    "\n",
    "# Evaluate on testing data using grid search  cv\n",
    "parameters = {'covariance_type':('full', 'spherical', 'diag', 'tied'), 'n_components':[8],'random_state':[0,19,42]}\n",
    "clf = GridSearchCV(GaussianMixture(), parameters, scoring=make_scorer(metrics.completeness_score))\n",
    "\n",
    "clf.fit(X_train_enc, y_train)\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_score_)\n",
    "\n",
    "parameters = {'init':('k-means++', 'random'), 'n_clusters':[8],'random_state':[0,19,42]}\n",
    "clf2 = GridSearchCV(KMeans(), parameters, scoring=make_scorer(metrics.completeness_score))\n",
    "\n",
    "clf2.fit(X_train_enc, y_train)\n",
    "print(clf2.best_estimator_)\n",
    "clf2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PCA \n",
    "# Reduce Dimensions\n",
    "# PCA \n",
    "pca1 = PCA(n_components=0.96)\n",
    "transformed_train = pca1.fit_transform(X_train_enc)\n",
    "\n",
    "pca2 = PCA(n_components=0.96)\n",
    "transformed_test = pca2.fit_transform(X_test_enc)\n",
    "\n",
    "# print(sum(pca1.explained_variance_ratio_))\n",
    "# print(sum(pca2.explained_variance_ratio_))\n",
    "print(pca1.n_components_)\n",
    "pca2.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
